{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb54b96",
   "metadata": {},
   "source": [
    "# Detecting marine heatwave based on observation\n",
    "\n",
    "## What is marine heatwaves\n",
    "The marine heatwaves are anomalous warm water over the ocean. \n",
    "To detect the warm ocean water, sea surface temperature (SST) is usually used to define if there is any marine heatwave event.\n",
    "\n",
    "### Goals in the notes\n",
    "- Extract the data from the PSL OPeNDAP server\n",
    "- Calculate the SST climatology\n",
    "- Calculate the SST anomaly \n",
    "- Determine the SST threshold based on the anomaly\n",
    "- Identify the marine heatwaves based on threshold \n",
    "\n",
    "## Extract the data from the PSL OPeNDAP server\n",
    "In this notebook, we demonstrate how to use the [NOAA OISST v2 High resolution dataset](https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.html) to show the observed marine heatwaves.\n",
    "The dataset is currently hosted on [NOAA Physical Sciences Laboratory](https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.html).\n",
    "\n",
    "````{margin}\n",
    "```{tip}\n",
    "To explore more gridded datasets that are hosted at NOAA PSL, here is a useful [search tool](https://psl.noaa.gov/data/gridded/index.html)\n",
    "```\n",
    "````\n",
    "\n",
    "```{note}\n",
    "The following example is following the paper [Jacox et al., 2022](http://doi.org/10.1038/s41586-022-04573-9). \n",
    "```\n",
    "\n",
    "### Python modules\n",
    "The module imported to the python kernel is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4caa7e00",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'intake'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mintake\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'intake'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import datetime\n",
    "import intake\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a100d0",
   "metadata": {},
   "source": [
    "But for the binder/jupyternotebook to work, these are the list of required packages\n",
    "```{note}\n",
    "- numpy\n",
    "- xarray\n",
    "- dask\n",
    "- netcdf4\n",
    "- h5netcdf\n",
    "- pydap\n",
    "- scipy\n",
    "- matplotlib\n",
    "- intake\n",
    "- intake-thredds\n",
    "```\n",
    "Some of the packages are used in the backend of the imported packaged shown above.\n",
    "\n",
    "\n",
    "### Utilize intake for thredds\n",
    "The thredds catalog can be viewed with the intake and intake-thredds module. \n",
    "This is not actually needed for downloading the data since PSL data server also provide the [OPeNDAP](https://www.earthdata.nasa.gov/engage/open-data-services-and-software/api/opendap) approach. \n",
    "Here, we are just demostrating a great way to generate a list of file names that can be seen in the thredds catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_url = 'https://psl.noaa.gov/thredds/catalog/Datasets/noaa.oisst.v2.highres/catalog.xml'\n",
    "catalog = intake.open_thredds_cat(cat_url, name='noaa-oisst')\n",
    "catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(catalog)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4210b",
   "metadata": {},
   "source": [
    "````{margin}\n",
    "```{tip}\n",
    "For more information, [intake-thredds readthedoc](https://intake-thredds.readthedocs.io/en/latest/tutorials/index.html) shows how to access data using dask from intake\n",
    "```\n",
    "````\n",
    "\n",
    "### Lazy loading the dataset through OPeNDAP\n",
    "With the power of [Xarray](https://docs.xarray.dev/en/stable/) and [Dask](https://www.dask.org), we are able to load the data lazily (only loading the meta data and coordinates information) and peek at data's dimension and availability on our local machine.\n",
    "The actual data (SST values at each grid point in this case) will only be downloaded from the PSL server when further data manipulation (subsetting and aggregation like calculating mean) is needed.\n",
    "The lazy loading approach provides the oppurtunity to reduce the memory usage in the calculation, the possibility of parallizing the processes, and side stepping the data download limit set by OPeNDAP server (PSL server has a 500MB limit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opendap_mon_url = \"https://psl.noaa.gov/thredds/dodsC/Datasets/noaa.oisst.v2.highres/sst.mon.mean.nc\"\n",
    "\n",
    "ds_mon = xr.open_dataset(opendap_mon_url, engine='pydap', chunks={'time':12,'lon':-1,'lat':-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e95705",
   "metadata": {},
   "source": [
    "```{important}\n",
    "The `chunks` keyward argument in the `open_dataset` is the key to your processing speed and how one avoid the download limit of OPeNDAP server. \n",
    "\n",
    "The `engine` keyward argument is set to `'pydap'` to utilize the pydap backend to grab the data on a OPeNDAP server.\n",
    "```\n",
    "#### What is chunk?\n",
    "Dask has a great [documentation](https://docs.dask.org/en/latest/array-chunks.html) of chunks. The basic idea is that a single netCDF file can be seperated into multiple chunks (e.g. 20(lon)x20(lat) into four chunks of 10(lon)x10(lat)). \n",
    "By reading each chunk at a time as needed, we do not have to load the entire dataset into the memory.\n",
    "This chunk by chunk reading is performed by Dask in the background. \n",
    "The only thing user need to do to activate this lazy loding approach is to read the netCDF file using the `xr.open_dataset()` method with the keyword argument `chunks={'time':12,'lon':-1,'lat':-1}` provided.\n",
    "The dataset is loaded lazily (only meta data and coordinates) shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43153102",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16bfad1",
   "metadata": {},
   "source": [
    "In our example, we set the size of each chunk to be 12(time)x1440(lon)x720(lat) which is equal to 47.46 MB of data while the entire dataset is 1.39 GB. \n",
    "This allow us to get data in 47.46 MB chunk per download request.\n",
    "\n",
    "## Calculate the SST climatology\n",
    "First, we need to define the period that we are going to use to calculate the climatology.\n",
    "Here, we picked 2010-2020 period to calculate the climatology.\n",
    "```{important}\n",
    "For a more accurate and scientificly valid estimate of marine heatwaves, one should usually consider a climatology period of at least 30 years.\n",
    "Here we set the climatology period from 2010 to 2020 (10years) to speed up the processing time and for demostration only. \n",
    "The shorter period (smaller memory consumption) also make the binder launch on this page available for user to manipulate and play with the dataset. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "climo_start_yr = 2010             # determine the climatology/linear trend start year\n",
    "climo_end_yr = 2020               # determine the climatology/linear trend end year\n",
    "\n",
    "ds_mon_crop = ds_mon.where((ds_mon['time.year']>=climo_start_yr)&\n",
    "                           (ds_mon['time.year']<=climo_end_yr),drop=True)\n",
    "\n",
    "ds_mon_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123785d7",
   "metadata": {},
   "source": [
    "To calculate the SST monthly climatology, we can utilize the `groupby` method from Xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mon_climo = ds_mon_crop.groupby('time.month').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656655a",
   "metadata": {},
   "source": [
    "## Calculate the SST anomaly\n",
    "After the climatology is determined, we substract the climatology from the original data to get the anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfb562",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mon_anom = (ds_mon_crop.groupby('time.month')-ds_mon_climo).compute()\n",
    "ds_mon_anom.sst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d30743",
   "metadata": {},
   "source": [
    "```{attention}\n",
    "Notice the `.compute()` method in the code above.\n",
    "The data of SST is only loaded in chunk, cropped to the desired period, averaged in group of month, and finally subtracted the climatology from the original data when we type compute.\n",
    "All these tasks are now executed in the background with distrbuted server assigning tasks to different CPUs. \n",
    "```\n",
    "\n",
    "## Determine the SST threshold based on the anomaly\n",
    "Normally, the monthly threshold can be calculated using a oneliner if the threshold is only based on the same month (e.g. January threshold is determined by all January SST anomaly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mon_anom.sst.groupby('time.month').quantile(0.95,dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef45709",
   "metadata": {},
   "source": [
    "However, based on the research, the threshold is determined based on a three month window with the center month being the threhold month one need to determined (e.g. January threshold is determined by all December, January, Feburary SST anomaly). \n",
    "Therefore, the function below is written to perform the three months window percentile operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Functions ######### \n",
    "# Function to calculate the 3 month rolling Quantile\n",
    "def mj_3mon_quantile(da_data, mhw_threshold=90.):\n",
    "    \n",
    "    da_data_quantile = xr.DataArray(coords={'lon':da_data.lon,\n",
    "                                            'lat':da_data.lat,\n",
    "                                            'month':np.arange(1,13)},\n",
    "                                    dims = ['month','lat','lon'])\n",
    "\n",
    "    for i in range(1,13):\n",
    "        if i == 1:\n",
    "            mon_range = [12,1,2]\n",
    "        elif i == 12 :\n",
    "            mon_range = [11,12,1]\n",
    "        else:\n",
    "            mon_range = [i-1,i,i+1]\n",
    "\n",
    "        da_data_quantile[i-1,:,:] = (da_data\n",
    "                                 .where((da_data['time.month'] == mon_range[0])|\n",
    "                                        (da_data['time.month'] == mon_range[1])|\n",
    "                                        (da_data['time.month'] == mon_range[2]),drop=True)\n",
    "                                 .quantile(mhw_threshold*0.01, dim = 'time', skipna = True))\n",
    "\n",
    "    return da_data_quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time da_mon_quantile = mj_3mon_quantile(ds_mon_anom.sst, mhw_threshold=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66efcf5a",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "the `%time` command is jupyter cell magic to time the one-liner cell operation which provide a great way to find bottleneck of your data processing steps.\n",
    "```\n",
    "\n",
    "The determined threshold value of each grid of each month is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a6348",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_mon_quantile.plot(col='month',vmin=0,vmax=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647341b6",
   "metadata": {},
   "source": [
    "## Identify the marine heatwaves based on threshold \n",
    "The figure below shows the original SST anomly value for the first 12 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34460d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mon_anom.sst.isel(time=slice(0,12)).plot(col='time',vmin=0,vmax=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a98932",
   "metadata": {},
   "source": [
    "To identify the marine heatwaves based on the monthly threshold that we just determined, we again use a one-liner code to find the monthly marine heatwaves with the grid that has SST anomaly below threshold to be masked as Not-a-Number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19514eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_mhw = ds_mon_anom.sst.where(ds_mon_anom.sst.groupby('time.month')>da_mon_quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69653e6",
   "metadata": {},
   "source": [
    "The figure below shows the SST anomly values that are above the monthly thresholds for the first 12 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca533aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_mhw.isel(time=slice(0,12)).plot(col='time',vmin=0,vmax=3)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.4"
   }
  },
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "source_map": [
   12,
   43,
   49,
   70,
   76,
   78,
   89,
   93,
   105,
   107,
   119,
   127,
   130,
   132,
   136,
   139,
   148,
   150,
   154,
   181,
   183,
   189,
   191,
   195,
   197,
   200,
   202,
   205
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}